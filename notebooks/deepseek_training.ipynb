{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview"
      },
      "source": [
        "# DeepSeek 7B für Deutsch trainieren\n",
        "\n",
        "Dieses Notebook führt folgende Schritte durch:\n",
        "1. DeepSeek 7B herunterladen\n",
        "2. Deutsche Trainingsdaten vorbereiten\n",
        "3. LoRA-Training für Deutsch durchführen\n",
        "4. Chinesische Token entfernen\n",
        "5. Modell quantisieren und exportieren\n",
        "\n",
        "**Hinweis:** Dieses Notebook ist für die Ausführung auf Google Colab Pro mit A100 GPU konzipiert."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. Umgebung einrichten und GPU prüfen\n",
        "\n",
        "Zuerst stellen wir sicher, dass wir Zugriff auf eine GPU haben und richten die Umgebung ein."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "source": [
        "# GPU prüfen\n",
        "!nvidia-smi"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "source": [
        "# Notwendige Pakete installieren\n",
        "!pip install -q transformers==4.36.2 datasets==2.15.0 peft==0.6.0 accelerate==0.24.1 bitsandbytes==0.41.1 torch==2.1.0 sentencepiece==0.1.99 gradio==3.50.2 wandb==0.15.12\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "import gc\n",
        "\n",
        "# Überprüfen, ob wir auf einer GPU laufen\n",
        "print(f\"CUDA verfügbar: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Modell: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Speicher: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_model"
      },
      "source": [
        "## 2. DeepSeek 7B Modell herunterladen\n",
        "\n",
        "Wir laden das DeepSeek-Coder 7B Basismodell, welches wir später für Deutsch optimieren werden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "source": [
        "# Modellname festlegen\n",
        "model_id = \"deepseek-ai/deepseek-coder-7b-base\"\n",
        "\n",
        "# Tokenizer laden\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "# Basiskonfigurationen für das Modell\n",
        "# Wir laden das Modell im 4-bit Quantisierungsmodus, um Speicherplatz zu sparen\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16, \n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=transformers.BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Wir überprüfen das Modell-Vokabular\n",
        "print(f\"Vokabulargröße: {len(tokenizer)}\")\n",
        "# Anzeigen einiger Beispieltokens\n",
        "print(\"Beispieltokens:\")\n",
        "for i in range(10):\n",
        "    print(f\"Token {i}: {tokenizer.decode([i])}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prepare_data"
      },
      "source": [
        "## 3. Deutsche Trainingsdaten vorbereiten\n",
        "\n",
        "Wir benötigen deutsche Texte für das Training. Dafür nutzen wir den Oscar Corpus, der hochwertige deutsche Textdaten enthält."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_german_data"
      },
      "source": [
        "# Laden eines deutschen Teildatensatzes aus dem Oscar Corpus\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Wir laden den deutschen Teil des Oscar Corpus\n",
        "# In der Produktion würden Sie mehr Daten verwenden\n",
        "dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_de\", split=\"train\", streaming=True)\n",
        "dataset = dataset.take(5000)  # Wir nehmen nur 5000 Beispiele für das Demo\n",
        "\n",
        "# Daten in eine Liste umwandeln\n",
        "german_texts = [item[\"text\"] for item in dataset]\n",
        "\n",
        "# Kurze Prüfung der Daten\n",
        "print(f\"Anzahl der Texte: {len(german_texts)}\")\n",
        "print(\"Beispieltext:\")\n",
        "print(german_texts[0][:500] + \"...\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tokenize_data"
      },
      "source": [
        "# Tokenisierung der Daten für das Training\n",
        "def tokenize_function(examples):\n",
        "    # Wir fügen spezielle Tokens hinzu, um das Format an das DeepSeek-Modell anzupassen\n",
        "    formatted_texts = [f\"<｜begin▁of▁sentence｜>{text}<｜end▁of▁sentence｜>\" for text in examples]\n",
        "    return tokenizer(formatted_texts, padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "# Tokenisierung der deutschen Texte\n",
        "tokenized_german_texts = tokenize_function(german_texts)\n",
        "\n",
        "# Erstellen eines HuggingFace Datasets für das Training\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_dict({\n",
        "    \"input_ids\": tokenized_german_texts[\"input_ids\"],\n",
        "    \"attention_mask\": tokenized_german_texts[\"attention_mask\"]\n",
        "})\n",
        "\n",
        "# Datensatz in Trainings- und Validierungsdaten aufteilen\n",
        "train_val_split = train_dataset.train_test_split(test_size=0.1)\n",
        "train_data = train_val_split[\"train\"]\n",
        "val_data = train_val_split[\"test\"]\n",
        "\n",
        "print(f\"Trainingsdaten: {len(train_data)} Beispiele\")\n",
        "print(f\"Validierungsdaten: {len(val_data)} Beispiele\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lora_training"
      },
      "source": [
        "## 4. LoRA-Training für Deutsch\n",
        "\n",
        "Wir verwenden LoRA (Low-Rank Adaptation), um das Modell effizient für Deutsch zu optimieren, ohne alle Parameter neu trainieren zu müssen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_lora"
      },
      "source": [
        "# Modell für LoRA-Training vorbereiten\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA-Konfiguration\n",
        "peft_config = LoraConfig(\n",
        "    r=16,  # Rang der LoRA-Matrix\n",
        "    lora_alpha=32,  # Skalierungsfaktor\n",
        "    lora_dropout=0.05,  # Dropout-Rate\n",
        "    bias=\"none\",  # Keine Bias-Parameter anpassen\n",
        "    task_type=\"CAUSAL_LM\",  # Aufgabentyp: Kausales Sprachmodell\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # Zielmodule für LoRA\n",
        ")\n",
        "\n",
        "# LoRA-Modell erstellen\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_training"
      },
      "source": [
        "# Trainingsargumente\n",
        "training_args = transformers.TrainingArguments(\n",
        "    output_dir=\"./deepseek-german-lora\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    save_steps=100,\n",
        "    logging_steps=10,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=100,\n",
        "    save_total_limit=3,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",  # Sie können \"wandb\" verwenden, wenn Sie Weights & Biases nutzen möchten\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        ")\n",
        "\n",
        "# Trainer initialisieren\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# Training starten\n",
        "trainer.train()\n",
        "\n",
        "# Modell speichern\n",
        "trainer.save_model(\"./deepseek-german-lora-final\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chinese_tokens"
      },
      "source": [
        "## 5. Chinesische Token entfernen\n",
        "\n",
        "Wir analysieren das Vokabular und bereiten die Entfernung der chinesischen Token vor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "analyze_tokens"
      },
      "source": [
        "# Identifizierung und Analyse der chinesischen Token im Vokabular\n",
        "def is_chinese_char(c):\n",
        "    \"\"\"Überprüft, ob ein Zeichen chinesisch ist.\"\"\"\n",
        "    cp = ord(c)\n",
        "    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  # CJK Unified Ideographs\n",
        "        (cp >= 0x3400 and cp <= 0x4DBF) or  # CJK Unified Ideographs Extension A\n",
        "        (cp >= 0x20000 and cp <= 0x2A6DF) or  # CJK Unified Ideographs Extension B\n",
        "        (cp >= 0x2A700 and cp <= 0x2B73F) or  # CJK Unified Ideographs Extension C\n",
        "        (cp >= 0x2B740 and cp <= 0x2B81F) or  # CJK Unified Ideographs Extension D\n",
        "        (cp >= 0x2B820 and cp <= 0x2CEAF) or  # CJK Unified Ideographs Extension E\n",
        "        (cp >= 0xF900 and cp <= 0xFAFF) or  # CJK Compatibility Ideographs\n",
        "        (cp >= 0x2F800 and cp <= 0x2FA1F)):  # CJK Compatibility Ideographs Supplement\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Analyse des Vokabulars\n",
        "token_counts = {\n",
        "    \"chinese\": 0,\n",
        "    \"non_chinese\": 0,\n",
        "    \"mixed\": 0\n",
        "}\n",
        "\n",
        "# Wir gehen durch die ersten 10.000 Token, um eine Übersicht zu bekommen\n",
        "for i in range(min(10000, len(tokenizer))):\n",
        "    token = tokenizer.decode([i])\n",
        "    chinese_chars = 0\n",
        "    total_chars = 0\n",
        "    \n",
        "    for c in token:\n",
        "        if c.strip():  # Nur nicht-Leerzeichen zählen\n",
        "            total_chars += 1\n",
        "            if is_chinese_char(c):\n",
        "                chinese_chars += 1\n",
        "    \n",
        "    if total_chars > 0:\n",
        "        chinese_ratio = chinese_chars / total_chars\n",
        "        if chinese_ratio > 0.8:\n",
        "            token_counts[\"chinese\"] += 1\n",
        "        elif chinese_ratio > 0:\n",
        "            token_counts[\"mixed\"] += 1\n",
        "        else:\n",
        "            token_counts[\"non_chinese\"] += 1\n",
        "\n",
        "print(\"Token-Analyse:\")\n",
        "print(f\"Chinesisch-dominierte Token: {token_counts['chinese']}\")\n",
        "print(f\"Gemischte Token: {token_counts['mixed']}\")\n",
        "print(f\"Nicht-chinesische Token: {token_counts['non_chinese']}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_token_removal"
      },
      "source": [
        "# Wir erstellen eine Liste der Token-IDs, die wir behalten möchten (nicht-chinesische und wichtige gemischte)\n",
        "retained_token_ids = []\n",
        "for i in range(len(tokenizer)):\n",
        "    token = tokenizer.decode([i])\n",
        "    chinese_chars = 0\n",
        "    total_chars = 0\n",
        "    \n",
        "    for c in token:\n",
        "        if c.strip():\n",
        "            total_chars += 1\n",
        "            if is_chinese_char(c):\n",
        "                chinese_chars += 1\n",
        "    \n",
        "    # Behalte Token, die nicht überwiegend chinesisch sind oder spezielle Token\n",
        "    if total_chars == 0 or chinese_chars / total_chars < 0.8 or i < 1000:  # Wichtige spezielle Token behalten\n",
        "        retained_token_ids.append(i)\n",
        "\n",
        "print(f\"Token behalten: {len(retained_token_ids)} von {len(tokenizer)}\")\n",
        "\n",
        "# Hinweis: Die vollständige Entfernung der Token würde eine Neuinitialisierung des Tokenizers erfordern,\n",
        "# was komplexer ist und hier nur vorbereitet wird. In der Praxis müsste man das Modell mit dem\n",
        "# bereinigten Vokabular neu trainieren."
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quantize_model"
      },
      "source": [
        "## 6. Modell quantisieren und exportieren\n",
        "\n",
        "Wir kombinieren das Basismodell mit den LoRA-Gewichten und quantisieren es für die Produktion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "merge_and_quantize"
      },
      "source": [
        "# Laden des trainierten LoRA-Modells\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Basismodell laden\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# LoRA-Modell laden und mit Basismodell kombinieren\n",
        "peft_model = PeftModel.from_pretrained(base_model, \"./deepseek-german-lora-final\")\n",
        "merged_model = peft_model.merge_and_unload()\n",
        "\n",
        "# 8-bit Quantisierungskonfiguration\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True\n",
        ")\n",
        "\n",
        "# Modell speichern\n",
        "merged_model.save_pretrained(\n",
        "    \"./deepseek-german-8bit\",\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "\n",
        "# Tokenizer speichern\n",
        "tokenizer.save_pretrained(\"./deepseek-german-8bit\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convert_to_gguf"
      },
      "source": [
        "# Optional: Konvertierung in GGUF-Format für llama.cpp\n",
        "# Llama.cpp klonen\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd llama.cpp && make\n",
        "\n",
        "# Konvertierung des Modells in das GGUF-Format\n",
        "!python llama.cpp/convert.py ./deepseek-german-8bit --outtype q4_0 --outfile ./deepseek-german-q4_0.gguf\n",
        "\n",
        "# Modell auf Google Drive speichern für den Download\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/KI-Projekt/models/deepseek-german\n",
        "!cp ./deepseek-german-q4_0.gguf /content/drive/MyDrive/KI-Projekt/models/deepseek-german/\n",
        "!cp -r ./deepseek-german-8bit /content/drive/MyDrive/KI-Projekt/models/deepseek-german/"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_model"
      },
      "source": [
        "## 7. Modell testen\n",
        "\n",
        "Wir testen das trainierte und quantisierte Modell mit einem deutschen Prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inference_test"
      },
      "source": [
        "# Laden des quantisierten Modells für Tests\n",
        "test_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"./deepseek-german-8bit\",\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "test_tokenizer = AutoTokenizer.from_pretrained(\"./deepseek-german-8bit\", trust_remote_code=True)\n",
        "\n",
        "# Beispielprompt\n",
        "prompt = \"<｜begin▁of▁sentence｜>Hallo, ich bin ein deutsch sprechender KI-Assistent. Wie kann ich dir heute helfen?<｜end▁of▁sentence｜>\"\n",
        "\n",
        "# Inferenz\n",
        "input_ids = test_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# Generierung\n",
        "with torch.no_grad():\n",
        "    output = test_model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=200,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "# Ausgabe decodieren\n",
        "generated_text = test_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Generierter Text:\")\n",
        "print(generated_text)"
      ],
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}